# README

## Project: Fine-Tuning a Small Language Model for Arabic Summarization

### Overview
This project focuses on fine-tuning a **Small Language Model (SLM)** for the task of **Arabic text summarization**. Using Google Colab, we leverage a combination of unannotated Arabic datasets, synthetic annotations generated by Large Language Models (LLMs), and state-of-the-art evaluation metrics to create a robust summarization model.

The project is structured into five key steps:
1. **Dataset Selection**: A dataset of 5,000 Arabic documents (e.g., from Arabic Wikipedia or news articles) is curated.
2. **Dataset Annotation**: High-quality summaries are generated for the documents using powerful LLMs like Atlas-Chat-27B or Qwen2.5-32B-Instruct, with quantization techniques applied to fit within Colab's hardware constraints.
3. **Data Splitting**: The annotated dataset is split into training, validation, and test sets to ensure proper model evaluation.
4. **Model Fine-tuning**: A lightweight SLM (e.g., Qwen2.5-0.5B-Instruct, Atlas-Chat-2B, or mT5-Base) is fine-tuned on the training set.
5. **Model Evaluation**: The fine-tuned model is evaluated on the test set using metrics such as **ROUGE**, **BERTScore**, and **LLM-as-a-Judge** to measure performance.

### Key Features
- **Language Focus**: Arabic (Modern Standard Arabic or Darija).
- **Efficient Workflow**: Designed to run on Google Colab's free-tier GPU with optimizations like quantization and batch processing.
- **State-of-the-Art Techniques**: Utilizes advanced LLMs for annotation and leverages popular evaluation metrics for robust assessment.
- **Scalability**: The protocol can be adapted for other languages or tasks with minimal changes.

### Deliverables
1. **Annotated Dataset**: A CSV/JSON file containing 5,000 Arabic documents and their corresponding synthetic summaries.
2. **Fine-tuned Model**: A checkpoint of the SLM fine-tuned for Arabic summarization.
3. **Evaluation Report**: Metrics and analysis of the model's performance on the test set.

### Requirements
- Python 3.x
- Libraries: `transformers`, `datasets`, `rouge-score`, `bert-score`, `bitsandbytes`, `accelerate`, `peft`
- Google Colab environment (free-tier GPU)

### Usage
1. Clone this repository and upload your dataset to Google Drive.
2. Run the provided Colab notebook step-by-step:
   - Step 1: Load and preprocess the dataset.
   - Step 2: Generate synthetic summaries using an LLM.
   - Step 3: Split the dataset into training, validation, and test sets.
   - Step 4: Fine-tune the SLM on the training set.
   - Step 5: Evaluate the model on the test set and analyze results.

### Acknowledgments
- **Datasets**: Arabic Wikipedia, OSCAR, TAC2011 Arabic Summarization Dataset.
- **Models**: Atlas-Chat-27B, Qwen2.5-32B-Instruct, mT5-Base, etc.
- **Tools**: Hugging Face Transformers, bitsandbytes for quantization, ROUGE, and BERTScore.

### Contact
For questions or feedback, please contact [Your Name] at [Your Email].

---

This README provides a concise summary of the project, its objectives, methodology, and deliverables, making it easy for others to understand and replicate the work.
