{
 cells [
  {
   cell_type markdown,
   metadata {},
   source [
    # Finetune Qwen-1.5B for Darija Summarization with Unslothn,
    ## Setup and Imports
   ]
  },
  {
   cell_type code,
   execution_count null,
   metadata {},
   source [
    !pip install unsloth[colab] @ git+httpsgithub.comunslothaiunsloth.gitn,
    !pip install -q datasets wandb
   ]
  },
  {
   cell_type code,
   execution_count null,
   metadata {},
   source [
    import torchn,
    from datasets import Datasetn,
    import pandas as pdn,
    import wandbn,
    from unsloth import FastLanguageModeln,
    from transformers import TrainingArguments
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    ## Configuration
   ]
  },
  {
   cell_type code,
   execution_count null,
   metadata {},
   source [
    class TrainingConfign,
        def __init__(self)n,
            self.model_id = QwenQwen-1_5Bn,
            self.output_dir = qwen-darija-summarizern,
            n,
            # Optimized for Unslothn,
            self.num_train_epochs = 3n,
            self.per_device_train_batch_size = 2n,
            self.gradient_accumulation_steps = 4n,
            self.learning_rate = 2e-5n,
            self.max_seq_length = 2048n,
            n,
            # LoRA confign,
            self.lora_r = 64n,
            self.lora_alpha = 32n,
            self.lora_dropout = 0.05n,
            n,
            # Datan,
            self.train_test_split = 0.1n,
    n,
    config = TrainingConfig()
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    ## Data Preparation
   ]
  },
  {
   cell_type code,
   execution_count null,
   metadata {},
   source [
    def load_dataset(file_path str) - Datasetn,
        df = pd.read_csv(file_path)n,
        return Dataset.from_dict({n,
            'text' df['original_text'].tolist(),n,
            'summary' df['summary'].tolist()n,
        })n,
    n,
    def format_prompt(text str, summary str) - strn,
        return fim_startusern,
    لخص النص التالي بالدارجة المغربية {text}im_endn,
    im_startassistantn,
    {summary}im_endn,
    n,
    def prepare_training_data(dataset Dataset, tokenizer) - Datasetn,
        def tokenize(examples)n,
            texts = [format_prompt(t, s) for t, s in zip(examples['text'], examples['summary'])]n,
            return tokenizer(n,
                texts,n,
                max_length=config.max_seq_length,n,
                padding=max_length,n,
                truncation=True,n,
                return_tensors=pt,n,
            )n,
        return dataset.map(tokenize, batched=True, remove_columns=dataset.column_names)
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    ## Model Preparation
   ]
  },
  {
   cell_type code,
   execution_count null,
   metadata {},
   source [
    def prepare_model()n,
        model, tokenizer = FastLanguageModel.from_pretrained(n,
            model_name=config.model_id,n,
            max_seq_length=config.max_seq_length,n,
            dtype=torch.float16,n,
            load_in_4bit=True,n,
        )n,
        n,
        model = FastLanguageModel.get_peft_model(n,
            model,n,
            r=config.lora_r,n,
            target_modules=[q_proj, k_proj, v_proj, o_proj],n,
            lora_alpha=config.lora_alpha,n,
            lora_dropout=config.lora_dropout,n,
            use_gradient_checkpointing=True,n,
        )n,
        return model, tokenizer
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    ## Training Setup
   ]
  },
  {
   cell_type code,
   execution_count null,
   metadata {},
   source [
    def setup_training()n,
        return TrainingArguments(n,
            output_dir=config.output_dir,n,
            num_train_epochs=config.num_train_epochs,n,
            per_device_train_batch_size=config.per_device_train_batch_size,n,
            gradient_accumulation_steps=config.gradient_accumulation_steps,n,
            learning_rate=config.learning_rate,n,
            logging_steps=10,n,
            save_strategy=steps,n,
            save_steps=100,n,
            evaluation_strategy=steps,n,
            eval_steps=100,n,
            report_to=wandb,n,
            fp16=not torch.cuda.is_bf16_supported(),n,
            bf16=torch.cuda.is_bf16_supported(),n,
            optim=paged_adamw_32bit,n,
            warmup_ratio=0.1,n,
        )
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    ## Training Pipeline
   ]
  },
  {
   cell_type code,
   execution_count null,
   metadata {},
   source [
    def train()n,
        wandb.login(key=your_api_key)n,
        wandb.init(project=qwen-darija-summarizer)n,
        n,
        # Load datan,
        dataset = load_dataset(summarized_documents.csv)n,
        dataset = dataset.train_test_split(test_size=config.train_test_split)n,
        n,
        # Prepare modeln,
        model, tokenizer = prepare_model()n,
        n,
        # Tokenize datasetsn,
        train_data = prepare_training_data(dataset['train'], tokenizer)n,
        eval_data = prepare_training_data(dataset['test'], tokenizer)n,
        n,
        # Get trainern,
        trainer = model.get_trainer(n,
            train_dataset=train_data,n,
            eval_dataset=eval_data,n,
            args=setup_training(),n,
        )n,
        n,
        # Start trainingn,
        trainer.train()n,
        n,
        # Save modeln,
        model.save_pretrained_merged(config.output_dir, tokenizer, save_method=merged_16bit)n,
        wandb.finish()
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    ## Run Training
   ]
  },
  {
   cell_type code,
   execution_count null,
   metadata {},
   source [
    if __name__ == __main__n,
        train()
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    ## Inference
   ]
  },
  {
   cell_type code,
   execution_count null,
   metadata {},
   source [
    def generate_summary(text str)n,
        model, tokenizer = FastLanguageModel.from_pretrained(n,
            config.output_dir,n,
            max_seq_length=config.max_seq_length,n,
            dtype=torch.float16,n,
        )n,
        n,
        prompt = fim_startusernلخص النص التالي بالدارجة المغربية {text}im_endnim_startassistantnn,
        n,
        inputs = tokenizer(prompt, return_tensors=pt, truncation=True).to(cuda)n,
        n,
        outputs = model.generate(n,
            inputs,n,
            max_new_tokens=256,n,
            temperature=0.7,n,
            repetition_penalty=1.2,n,
        )n,
        n,
        return tokenizer.decode(outputs[0], skip_special_tokens=True)n,
    n,
    # Usagen,
    print(generate_summary(ضع هنا النص المغربي الذي تريد تلخيصه...))
   ]
  }
 ],
 metadata {
  kernelspec {
   display_name Python 3,
   language python,
   name python3
  },
  language_info {
   name python,
   version 3.10
  }
 },
 nbformat 4,
 nbformat_minor 4
}