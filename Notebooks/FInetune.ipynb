{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10864270,"sourceType":"datasetVersion","datasetId":6749331}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Finetune Qwen-1.5B for Moroccan Darija Summarization\n## Setup and Imports","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers datasets accelerate peft bitsandbytes wandb","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom datasets import Dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments\n)\nfrom peft import (\n    prepare_model_for_kbit_training,\n    LoraConfig,\n    get_peft_model\n)\nimport pandas as pd\nimport wandb\nfrom typing import Dict, List","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Configuration","metadata":{}},{"cell_type":"code","source":"class TrainingConfig:\n    def __init__(self):\n        self.model_id = \"Qwen/Qwen-1_5B\"\n        self.output_dir = \"qwen-darija-summarizer\"\n        \n        # Training params\n        self.num_train_epochs = 3\n        self.per_device_train_batch_size = 4\n        self.gradient_accumulation_steps = 4\n        self.learning_rate = 2e-4\n        self.max_grad_norm = 0.3\n        self.max_steps = -1\n        self.warmup_ratio = 0.03\n        self.lr_scheduler_type = \"cosine\"\n        \n        # LoRA params\n        self.lora_r = 64\n        self.lora_alpha = 16\n        self.lora_dropout = 0.1\n        \n        # Data processing\n        self.max_length = 512\n        self.train_test_split = 0.1\n        \nconfig = TrainingConfig()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"code","source":"def load_dataset(file_path: str) -> Dataset:\n    \"\"\"Load and prepare the dataset from the summarization results\"\"\"\n    df = pd.read_csv(file_path)\n    \n    # Create training pairs\n    dataset_dict = {\n        'text': df['original_text'].tolist(),\n        'summary': df['summary'].tolist()\n    }\n    \n    return Dataset.from_dict(dataset_dict)\n\ndef prepare_training_data(dataset: Dataset, tokenizer) -> Dataset:\n    \"\"\"Prepare the dataset for training by formatting and tokenizing\"\"\"\n    def format_prompt(text: str, summary: str) -> str:\n        return f\"Summarize the following text in Moroccan Darija:\\n{text}\\n\\nSummary:\\n{summary}\"\n    \n    def tokenize(examples):\n        prompts = [format_prompt(text, summary) \n                   for text, summary in zip(examples['text'], examples['summary'])]\n        \n        return tokenizer(\n            prompts,\n            truncation=True,\n            max_length=config.max_length,\n            padding='max_length',\n            return_tensors='pt'\n        )\n    \n    tokenized_dataset = dataset.map(\n        tokenize,\n        batched=True,\n        remove_columns=dataset.column_names\n    )\n    \n    return tokenized_dataset","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Preparation","metadata":{}},{"cell_type":"code","source":"def prepare_model():\n    \"\"\"Initialize and prepare the model for training\"\"\"\n    # Quantization config\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_use_double_quant=True\n    )\n    \n    # Load model and tokenizer\n    model = AutoModelForCausalLM.from_pretrained(\n        config.model_id,\n        quantization_config=bnb_config,\n        device_map=\"auto\"\n    )\n    tokenizer = AutoTokenizer.from_pretrained(config.model_id)\n    \n    # Prepare for k-bit training\n    model = prepare_model_for_kbit_training(model)\n    \n    # LoRA configuration\n    lora_config = LoraConfig(\n        r=config.lora_r,\n        lora_alpha=config.lora_alpha,\n        lora_dropout=config.lora_dropout,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\"\n    )\n    \n    # Get PEFT model\n    model = get_peft_model(model, lora_config)\n    \n    return model, tokenizer","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training Setup","metadata":{}},{"cell_type":"code","source":"def setup_training(tokenized_dataset):\n    \"\"\"Setup training arguments and initialize trainer\"\"\"\n    training_args = TrainingArguments(\n        output_dir=config.output_dir,\n        num_train_epochs=config.num_train_epochs,\n        per_device_train_batch_size=config.per_device_train_batch_size,\n        gradient_accumulation_steps=config.gradient_accumulation_steps,\n        learning_rate=config.learning_rate,\n        max_grad_norm=config.max_grad_norm,\n        max_steps=config.max_steps,\n        warmup_ratio=config.warmup_ratio,\n        lr_scheduler_type=config.lr_scheduler_type,\n        fp16=True,\n        logging_steps=10,\n        save_strategy=\"steps\",\n        save_steps=100,\n        evaluation_strategy=\"steps\",\n        eval_steps=100,\n        report_to=\"wandb\"\n    )\n    \n    return training_args","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training Pipeline","metadata":{}},{"cell_type":"code","source":"def train():\n    # Initialize wandb\n    wandb.init(project=\"qwen-darija-summarizer\")\n    \n    # Load and prepare dataset\n    dataset = load_dataset(\"summarized_documents.csv\")\n    \n    # Train/test split\n    dataset = dataset.train_test_split(test_size=config.train_test_split)\n    \n    # Prepare model and tokenizer\n    model, tokenizer = prepare_model()\n    \n    # Prepare datasets\n    train_dataset = prepare_training_data(dataset['train'], tokenizer)\n    eval_dataset = prepare_training_data(dataset['test'], tokenizer)\n    \n    # Setup training arguments\n    training_args = setup_training(train_dataset)\n    \n    # Initialize Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset\n    )\n    \n    # Train\n    trainer.train()\n    \n    # Save model\n    trainer.save_model()\n    \n    wandb.finish()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Run Training","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    train()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inference Example","metadata":{}},{"cell_type":"code","source":"def generate_summary(text: str, model_path: str):\n    # Load fine-tuned model and tokenizer\n    model = AutoModelForCausalLM.from_pretrained(model_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    \n    prompt = f\"Summarize the following text in Moroccan Darija:\\n{text}\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=config.max_length)\n    \n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=200,\n        temperature=0.7,\n        num_return_sequences=1\n    )\n    \n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Example usage\ntest_text = \"Your test text here...\"\nsummary = generate_summary(test_text, config.output_dir)\nprint(summary)","metadata":{},"outputs":[],"execution_count":null}]}